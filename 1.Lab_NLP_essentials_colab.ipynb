{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"colab":{"name":"1.Lab_NLP_essentials_colab.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"KgEZZozntwbX"},"source":["# NLP Workflow"]},{"cell_type":"code","metadata":{"id":"neDYjApFtwbY"},"source":["sentence = \"I am a master student. I love natural language processing.\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ApwKsCBItwbc"},"source":["### Tokenization"]},{"cell_type":"code","metadata":{"id":"UBJrisCgufqR"},"source":["!pip install nltk"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A082BH2pADji"},"source":["import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pnyYZ2EQtwbd"},"source":["from nltk.tokenize import sent_tokenize\n","\n","sentence = \"I am a master student. I love natural language processing.\"\n","documents = sent_tokenize(sentence)\n","documents"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9N0p7WoYtwbj"},"source":["from nltk.tokenize import word_tokenize\n","from string import punctuation\n","\n","sentence = \"I am a master student. I love natural language processing.\"\n","sentence = ''.join([c for c in sentence if c not in punctuation])\n","word_token=word_tokenize(sentence)\n","word_token"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fFbx1OJUtwbn"},"source":["### Filtering Stop Words"]},{"cell_type":"code","metadata":{"id":"bs_tnug5twbl"},"source":["from nltk.corpus import stopwords\n","\n","stop_words = set(stopwords.words('english'))\n","filtered_sentence = [w for w in word_token if not w in stop_words]\n","print(filtered_sentence)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eSs1R5P5twbo"},"source":["unique_words = set(filtered_sentence)\n","unique_words"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ooviuzYotwbq"},"source":["## Vectorization"]},{"cell_type":"code","metadata":{"id":"M_uuG-jZtwbr"},"source":["import pandas as pd"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DUtkO9ettwbt"},"source":["def tokenization(doc):\n","    doc = ''.join([c for c in doc if c not in punctuation])\n","    doc = doc.split(' ')\n","    doc = [w for w in doc if not w in stop_words]\n","    return doc"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NgzDzZSHtwbu"},"source":["document1 = tokenization(documents[0])\n","document2 = tokenization(documents[1])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cFgxZA1Rtwby"},"source":["### Bag of Words"]},{"cell_type":"code","metadata":{"id":"Yr-o5YxBtwbz"},"source":["num_of_words1 = dict.fromkeys(unique_words, 0)\n","for word in document1:\n","    num_of_words1[word] += 1\n","\n","num_of_words2 = dict.fromkeys(unique_words, 0)\n","for word in document2:\n","    num_of_words2[word] += 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oJCs3M-gtwb1"},"source":["pd.DataFrame([num_of_words1,num_of_words2])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PVnNMEM5twb3"},"source":["### N-grams"]},{"cell_type":"code","metadata":{"id":"vqchtVWstwb3"},"source":["def ngrams(doc,n):\n","    ngrams = zip(*[doc[i:] for i in range(n)])\n","    return [\" \".join(ngram) for ngram in ngrams]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0efnfFPTtwb5"},"source":["# bigram (2-gram)\n","ngrams1 = ngrams(document1,2)\n","ngrams2 = ngrams(document2,2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8FbDnhwltwb7"},"source":["ngrams1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"apJozvpitwb9"},"source":["ngrams2"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3u7l023Utwb_"},"source":["### TF-IDF\n","Tutorial: https://towardsdatascience.com/natural-language-processing-feature-engineering-using-tf-idf-e8b9d00e7e76\n","\n","#### Term Frequency (TF)"]},{"cell_type":"code","metadata":{"id":"jgFRRJ5StwcA"},"source":["def compute_TF(word_dict, bag_of_words):\n","    tf_dict = {}\n","    words_count = len(bag_of_words)\n","    for word, count in word_dict.items():\n","        tf_dict[word] = count / float(words_count)\n","    return tf_dict"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Mvzdv31QtwcE"},"source":["tf1 = compute_TF(num_of_words1, document1)\n","tf2 = compute_TF(num_of_words2, document2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aNQejAWKtwcF"},"source":["pd.DataFrame([tf1,tf2])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jct1JIP0twcH"},"source":["#### Inverse Data Frequency (IDF)"]},{"cell_type":"code","metadata":{"id":"O8rznWortwcI"},"source":["import math\n","def compute_IDF(documents):\n","    n = len(documents)\n","    idf_dict = dict.fromkeys(documents[0].keys(), 0)\n","    for doc in documents:\n","        for word, val in doc.items():\n","            if val > 0:\n","                idf_dict[word] += 1\n","    for word, val in idf_dict.items():\n","        idf_dict[word] = math.log(n/float(val))\n","    return idf_dict"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dArZY2ANtwcM"},"source":["idf = compute_IDF([num_of_words1, num_of_words2])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VeYbGXPAtwcP"},"source":["idf"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WsathQnLtwcT"},"source":["pd.Series(idf).to_frame().T"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y6zMBGRQtwcV"},"source":["#### Compute TF-IDF\n","TF-IDF = Term Frequency (TF) * Inverse Document Frequency (IDF)"]},{"cell_type":"code","metadata":{"id":"b-4uiK6RtwcV"},"source":["def compute_TFIDF(tfs, idfs):\n","    tfidf = {}\n","    for word, val in tfs.items():\n","        tfidf[word] = val * idfs[word]\n","    return tfidf"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U35-F3_htwcX"},"source":["tfidf1 = compute_TFIDF(tf1, idf)\n","tfidf2 = compute_TFIDF(tf2, idf)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r9WzlffZtwcc"},"source":["pd.DataFrame([tfidf1,tfidf2])"],"execution_count":null,"outputs":[]}]}